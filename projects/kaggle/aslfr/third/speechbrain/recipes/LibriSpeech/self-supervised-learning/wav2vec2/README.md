# wav2vec 2.0 pretraining with SpeechBrain

This folder contains the scripts to train a wav2vec2 based system using LibriSpeech. It can be adapted to any dataset as long as you provide the csv or json files as with other recipes. No other adaptation will be required apart from controlling the sequence length and Dynamic Batching arguments to avoid out of memory issues.

# SpeechBrain VS HuggingFace wav2vec2 training ??
As usual, our goal at SpeechBrain remains to offer as much flexibility to the user as possible. Hence, wav2vec2 pretraining can be achieved in two different ways: fully with SpeechBrain, or following our HuggingFace interface. Both approaches give similar results. Indeed we tested both with a BASE model pretrained on LibriSpeech and fine-tuned on LibriSpeech for ASR, IEMOCAP for emotion recognition and VoxCeleb 1 for speaker identification. Therefore, it is up to the user to decide what training scheme he/she wish to follow. A full SpeechBrain training offers a unique flexibility for further research (e.g. changing the loss, changing the architecture, modifying absolutely everything with wav2vec2), while the HuggingFace pretraining offers a good interfacing with the transformers library.


**On LibriSpeech, we officialy provide only a fully SpeechBrain recipe. If you wish to use the HuggingFace pretraining, please go to our [CommonVoice recipe](https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonVoice/self-supervised-learning/wav2vec2)**

# Go !
Simply type:
```shell
python train_sb_wav2vec2.py hparams/wav2vec2_base.yaml
```

Do not forget to replace the `!PLACEHOLDER` variables in the yaml corresponding to your local configuration.

# Use a pretrained model for fine-tuning with SpeechBrain

The checkpoint generated by this pretraining is a standard PyTorch checkpoint. If you wish to use it as any pretrained model, simply follow our [fine-tuning exemple using our dedicated Pretrainer](#).

# Results

| Release | Hyperparams file | Pre-Training Dataset | Finetuning Dataset | WER | HuggingFace link | Full model link | GPUs |
|:-------------:|:---------------------------:|:---------------------------:| :-----:| :-----:| :-----:| :-----:| :--------:|
| 22-09-22 | wav2vec2_base.yaml | LibriSpeech 960h | LibriSpeech 100h | 7.X (LibriSpeech test-clean) | [Link](https://huggingface.co/speechbrain/ssl-wav2vec2-base-librispeech) | [Link](https://www.dropbox.com/sh/y88z33qtgbl49k4/AAAcVxaBjTh5W_HH99D5UKmka?dl=0) | 16xTesla V100 32GB |

# Advices
Training wav2vec 2.0 models is crazy w.r.t compute resources. For instance, this recipe only trains a BASE wav2vec 2.0 architecture, and it already requires 16 Tesla V100 for 7 days. Of course, you can scale this to your needs (e.g., you can work with 2 GPUs only), but it will take ages! Welcome to the wav2vec 2.0 world!

Here is a list of the most important advices:
- To train w2v2 models, it is **extremely** important to have an effective batch size as high as possible. For instance, the original BASE model is trained with batches containing 1.6H of speech. This means that (duration_per_minibatch * nb_gpu * gradient_accumulation) must be at least equal to 1.6H.
- Do not train on sequences longer than 20s, this will blow your VRAM up and is useless for now. Indeed training with shorter sentences (10s) may work just as well.
- Set the `n_warmup_steps` steps in such a way that it corresponds to 10% of the total training steps. The number of steps correspond to the actual number of call to .backward w.r.t the batch size.
