#!/usr/bin/env python 
# -*- coding: utf-8 -*-
# ==============================================================================
#          \file   dataset.py
#        \author   chenghuige  
#          \date   2023-06-26 17:16:21.385835
#   \Description  
# ==============================================================================

  
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from gezi.common import * 
from src.config import *
from src.tf.preprocess import PreProcssor
from src.tf.augment import cutmix
import melt as mt

class Dataset(mt.Dataset):
  def __init__(self, subset='valid', **kwargs):
    super(Dataset, self).__init__(subset, **kwargs)
    assert not FLAGS.batch_parse, 'PreprocessLayer will be used as FLAGS.batch_parse = False'
    self.prepocess = PreProcssor(subset)

  def parse(self, example):
    dynamic_keys = ['frames']
    self.auto_parse(exclude_keys=dynamic_keys)
    self.adds(dynamic_keys)
    fe = self.parse_(serialized=example)
    return self.prepocess(fe)
    
# TODO why much more slower... encode-small 2.2s/it
# TODO 因为现在本机intel还是比较慢相对autodl的amd ParseExampleV2 是瓶颈 可能还是硬盘读取缓存有差异？ loop dataset慢很多 
# TODO 尝试都用torch dataset 包括preprocess也纯torch操作 纯in memory的dataset 再对比下速度 当前encode-small 6.8it/s
# amd上面差不多10.2it/s, amd上面测试瓶颈不是数据读取
def get_dataset(df, subset):
  if subset != 'train':
    df = df[df.fold == FLAGS.fold]
    FLAGS.num_valid = len(df)
  else:
    if not FLAGS.online:
      df = df[df.fold != FLAGS.fold]
    FLAGS.num_train = len(df)
  from datasets import Dataset
  ds = Dataset.from_pandas(df)
  ds = ds.to_tf_dataset(batch_size=1)
  preprocesor = PreProcssor(subset, squeeze=True)
  ds = ds.map(preprocesor, tf.data.AUTOTUNE)
  if subset == 'train':
    ds = ds.shuffle(FLAGS.buffer_size)
    ## shuffle all in memory
    # ds = ds.shuffle(ds.cardinality())
  options = tf.data.Options()
  options.experimental_deterministic = (False)
  ds = ds.with_options(options)
  if subset == 'train':
    ds = ds.batch(FLAGS.batch_size, drop_remainder=True)
  else:
    ds = ds.batch(FLAGS.eval_batch_size, drop_remainder=False)
  ds = ds.prefetch(tf.data.AUTOTUNE)
  return ds

def get_datasets():
  # train.fea is generated by prepare/gen-df.py
  df = pd.read_feather(f'{FLAGS.root}/train.fea')
  train_ds = get_dataset(df, 'train')
  eval_ds = get_dataset(df, 'eval')
  return train_ds, eval_ds
