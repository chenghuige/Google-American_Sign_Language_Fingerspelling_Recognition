{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:47:09.611312Z","iopub.status.busy":"2023-06-12T09:47:09.610792Z","iopub.status.idle":"2023-06-12T09:47:19.23526Z","shell.execute_reply":"2023-06-12T09:47:19.233915Z","shell.execute_reply.started":"2023-06-12T09:47:09.611261Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import json\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:48:32.183916Z","iopub.status.busy":"2023-06-12T09:48:32.18339Z","iopub.status.idle":"2023-06-12T09:48:32.327438Z","shell.execute_reply":"2023-06-12T09:48:32.325791Z","shell.execute_reply.started":"2023-06-12T09:48:32.183874Z"},"trusted":true},"outputs":[],"source":["with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    char_to_num = json.load(f)\n","\n","num_to_char = {j:i for i,j in char_to_num.items()}\n","\n","inpdir = \"/kaggle/input/asl-fingerspelling\"\n","df = pd.read_csv(f'{inpdir}/train.csv')\n","\n","LPOSE = [13, 15, 17, 19, 21]\n","RPOSE = [14, 16, 18, 20, 22]\n","POSE = LPOSE + RPOSE\n","\n","RHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\n","LHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\n","POSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n","\n","X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\n","Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\n","Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n","\n","SEL_COLS = X + Y + Z\n","FRAME_LEN = 128\n","\n","X_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\n","Y_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\n","Z_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n","\n","RHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\n","LHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\n","RPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\n","LPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:52:29.108292Z","iopub.status.busy":"2023-06-12T09:52:29.107784Z","iopub.status.idle":"2023-06-12T09:52:29.412573Z","shell.execute_reply":"2023-06-12T09:52:29.411457Z","shell.execute_reply.started":"2023-06-12T09:52:29.108258Z"},"trusted":true},"outputs":[],"source":["def resize_pad(x):\n","    if tf.shape(x)[0] < FRAME_LEN:\n","        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n","    else:\n","        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n","    return x\n","\n","def pre_process(x):\n","    rhand = tf.gather(x, RHAND_IDX, axis=1)\n","    lhand = tf.gather(x, LHAND_IDX, axis=1)\n","    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n","    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n","    \n","    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n","    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n","    \n","    rnans = tf.math.count_nonzero(rnan_idx)\n","    lnans = tf.math.count_nonzero(lnan_idx)\n","    \n","    # For dominant hand\n","    if rnans > lnans:\n","        hand = lhand\n","        pose = lpose\n","        \n","        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n","        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n","        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n","        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n","        \n","        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n","        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n","        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n","        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n","    else:\n","        hand = rhand\n","        pose = rpose\n","    \n","    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n","    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n","    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n","    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n","    \n","    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n","    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n","    hand = (hand - mean) / std\n","\n","    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n","    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n","    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n","    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n","    \n","    x = tf.concat([hand, pose], axis=1)\n","    x = resize_pad(x)\n","    \n","    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n","    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n","    return x\n","\n","def load_relevant_data_subset(pq_path):\n","    return pd.read_parquet(pq_path, columns=SEL_COLS)\n","\n","file_id = df.file_id.iloc[0]\n","inpdir = \"/kaggle/input/asl-fingerspelling\"\n","pqfile = f\"{inpdir}/train_landmarks/{file_id}.parquet\"\n","seq_refs = df.loc[df.file_id == file_id]\n","seqs = load_relevant_data_subset(pqfile)\n","\n","seq_id = seq_refs.sequence_id.iloc[0]\n","frames = seqs.iloc[seqs.index == seq_id]\n","phrase = str(df.loc[df.sequence_id == seq_id].phrase.iloc[0])\n","\n","print(pre_process(frames).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:55:12.273982Z","iopub.status.busy":"2023-06-12T09:55:12.273436Z","iopub.status.idle":"2023-06-12T09:55:13.832464Z","shell.execute_reply":"2023-06-12T09:55:13.831222Z","shell.execute_reply.started":"2023-06-12T09:55:12.273943Z"},"trusted":true},"outputs":[],"source":["table = tf.lookup.StaticHashTable(\n","    initializer=tf.lookup.KeyValueTensorInitializer(\n","        keys=list(char_to_num.keys()),\n","        values=list(char_to_num.values()),\n","    ),\n","    default_value=tf.constant(-1),\n","    name=\"class_weight\"\n",")\n","\n","mask_idx = char_to_num['#']\n","\n","def preprocess_fn(landmarks, phrase):\n","    phrase = ';' + phrase + '['\n","    phrase = tf.strings.bytes_split(phrase)\n","    phrase = table.lookup(phrase)\n","    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], constant_values=mask_idx)\n","    return pre_process(landmarks), phrase\n","\n","def decode_fn(record_bytes):\n","    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n","    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n","    features = tf.io.parse_single_example(record_bytes, schema)\n","    phrase = features[\"phrase\"]\n","    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n","    landmarks = tf.transpose(landmarks)\n","    return landmarks, phrase\n","\n","inpdir = \"/kaggle/input/aslfr-preprocess\"\n","tffiles = df.file_id.map(lambda x: f'{inpdir}/train/{x}.tfrecord').unique()\n","\n","batch_size = 32\n","val_len = int(0.1 * len(tffiles))\n","\n","train_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(buffer_size=500).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","val_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","test_dataset = tf.data.TFRecordDataset(tffiles).map(decode_fn).prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:55:51.841186Z","iopub.status.busy":"2023-06-12T09:55:51.840703Z","iopub.status.idle":"2023-06-12T09:55:51.853742Z","shell.execute_reply":"2023-06-12T09:55:51.85251Z","shell.execute_reply.started":"2023-06-12T09:55:51.841153Z"},"trusted":true},"outputs":[],"source":["class TokenEmbedding(layers.Layer):\n","    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n","        super().__init__()\n","        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        x = self.emb(x)\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        return x + positions\n","\n","\n","class LandmarkEmbedding(layers.Layer):\n","    def __init__(self, num_hid=64, maxlen=100):\n","        super().__init__()\n","        self.conv1 = tf.keras.layers.Conv1D(\n","            num_hid, 11, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv2 = tf.keras.layers.Conv1D(\n","            num_hid, 11, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv3 = tf.keras.layers.Conv1D(\n","            num_hid, 11, padding=\"same\", activation=\"relu\"\n","        )\n","\n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:55:55.371906Z","iopub.status.busy":"2023-06-12T09:55:55.371419Z","iopub.status.idle":"2023-06-12T09:55:55.383875Z","shell.execute_reply":"2023-06-12T09:55:55.382295Z","shell.execute_reply.started":"2023-06-12T09:55:55.371872Z"},"trusted":true},"outputs":[],"source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [\n","                layers.Dense(feed_forward_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:56:01.774538Z","iopub.status.busy":"2023-06-12T09:56:01.774084Z","iopub.status.idle":"2023-06-12T09:56:01.79201Z","shell.execute_reply":"2023-06-12T09:56:01.790385Z","shell.execute_reply.started":"2023-06-12T09:56:01.774503Z"},"trusted":true},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n","        super().__init__()\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n","        self.self_att = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.self_dropout = layers.Dropout(0.5)\n","        self.enc_dropout = layers.Dropout(0.1)\n","        self.ffn_dropout = layers.Dropout(0.1)\n","        self.ffn = keras.Sequential(\n","            [\n","                layers.Dense(feed_forward_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","\n","    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n","        \"\"\"Masks the upper half of the dot product matrix in self attention.\n","\n","        This prevents flow of information from future tokens to current token.\n","        1's in the lower triangle, counting from the lower right corner.\n","        \"\"\"\n","        i = tf.range(n_dest)[:, None]\n","        j = tf.range(n_src)\n","        m = i >= j - n_src + n_dest\n","        mask = tf.cast(m, dtype)\n","        mask = tf.reshape(mask, [1, n_dest, n_src])\n","        mult = tf.concat(\n","            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n","        )\n","        return tf.tile(mask, mult)\n","\n","    def call(self, enc_out, target):\n","        input_shape = tf.shape(target)\n","        batch_size = input_shape[0]\n","        seq_len = input_shape[1]\n","        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n","        target_att = self.self_att(target, target, attention_mask=causal_mask)\n","        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n","        enc_out = self.enc_att(target_norm, enc_out)\n","        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n","        ffn_out = self.ffn(enc_out_norm)\n","        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n","        return ffn_out_norm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:56:40.184616Z","iopub.status.busy":"2023-06-12T09:56:40.184106Z","iopub.status.idle":"2023-06-12T09:56:40.214674Z","shell.execute_reply":"2023-06-12T09:56:40.213239Z","shell.execute_reply.started":"2023-06-12T09:56:40.184582Z"},"trusted":true},"outputs":[],"source":["class Transformer(keras.Model):\n","    def __init__(\n","        self,\n","        num_hid=64,\n","        num_head=2,\n","        num_feed_forward=128,\n","        source_maxlen=100,\n","        target_maxlen=100,\n","        num_layers_enc=4,\n","        num_layers_dec=1,\n","        num_classes=10,\n","    ):\n","        super().__init__()\n","        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n","        self.acc_metric = keras.metrics.Mean(name=\"edit_dist\")\n","        self.num_layers_enc = num_layers_enc\n","        self.num_layers_dec = num_layers_dec\n","        self.target_maxlen = target_maxlen\n","        self.num_classes = num_classes\n","\n","        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n","        self.dec_input = TokenEmbedding(\n","            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n","        )\n","\n","        self.encoder = keras.Sequential(\n","            [self.enc_input]\n","            + [\n","                TransformerEncoder(num_hid, num_head, num_feed_forward)\n","                for _ in range(num_layers_enc)\n","            ]\n","        )\n","\n","        for i in range(num_layers_dec):\n","            setattr(\n","                self,\n","                f\"dec_layer_{i}\",\n","                TransformerDecoder(num_hid, num_head, num_feed_forward),\n","            )\n","\n","        self.classifier = layers.Dense(num_classes)\n","\n","    def decode(self, enc_out, target):\n","        y = self.dec_input(target)\n","        for i in range(self.num_layers_dec):\n","            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n","        return y\n","\n","    def call(self, inputs):\n","        source = inputs[0]\n","        target = inputs[1]\n","        x = self.encoder(source)\n","        y = self.decode(x, target)\n","        return self.classifier(y)\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_metric, self.acc_metric]\n","\n","    def train_step(self, batch):\n","        \"\"\"Processes one batch inside model.fit().\"\"\"\n","        source = batch[0]\n","        target = batch[1]\n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        with tf.GradientTape() as tape:\n","            preds = self([source, dec_input])\n","            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","            mask = tf.math.logical_not(tf.math.equal(dec_target, mask_idx))\n","            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        self.loss_metric.update_state(loss)\n","        \n","        edit_dist = tf.edit_distance(tf.sparse.from_dense(target), tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n","        edit_dist = tf.reduce_mean(edit_dist)\n","        self.acc_metric.update_state(edit_dist)\n","        \n","        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n","\n","    def test_step(self, batch):\n","        source = batch[0]\n","        target = batch[1]\n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        preds = self([source, dec_input])\n","        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","        mask = tf.math.logical_not(tf.math.equal(dec_target, mask_idx))\n","        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n","        self.loss_metric.update_state(loss)\n","        \n","        edit_dist = tf.edit_distance(tf.sparse.from_dense(target), tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n","        edit_dist = tf.reduce_mean(edit_dist)\n","        self.acc_metric.update_state(edit_dist)\n","        \n","        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n","\n","    def generate(self, source, target_start_token_idx):\n","        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n","        bs = tf.shape(source)[0]\n","        enc = self.encoder(source)\n","        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n","        dec_logits = []\n","        for i in range(self.target_maxlen - 1):\n","            dec_out = self.decode(enc, dec_input)\n","            logits = self.classifier(dec_out)\n","            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","            #last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n","            last_logit = logits[:, -1][..., tf.newaxis]\n","            dec_logits.append(last_logit)\n","            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n","        return dec_input"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:57:16.441993Z","iopub.status.busy":"2023-06-12T09:57:16.441328Z","iopub.status.idle":"2023-06-12T09:57:16.745804Z","shell.execute_reply":"2023-06-12T09:57:16.744901Z","shell.execute_reply.started":"2023-06-12T09:57:16.44194Z"},"trusted":true},"outputs":[],"source":["class DisplayOutputs(keras.callbacks.Callback):\n","    def __init__(\n","        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n","    ):\n","        \"\"\"Displays a batch of outputs after every epoch\n","\n","        Args:\n","            batch: A test batch containing the keys \"source\" and \"target\"\n","            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n","            target_start_token_idx: A start token index in the target vocabulary\n","            target_end_token_idx: An end token index in the target vocabulary\n","        \"\"\"\n","        self.batch = batch\n","        self.target_start_token_idx = target_start_token_idx\n","        self.target_end_token_idx = target_end_token_idx\n","        self.idx_to_char = idx_to_token\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","#         if epoch % 5 != 0:\n","#             return\n","        source = self.batch[0]\n","        target = self.batch[1].numpy()\n","        bs = tf.shape(source)[0]\n","        preds = self.model.generate(source, self.target_start_token_idx)\n","        preds = preds.numpy()\n","        for i in range(bs):\n","            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n","            prediction = \"\"\n","            for idx in preds[i, :]:\n","                prediction += self.idx_to_char[idx]\n","                if idx == self.target_end_token_idx:\n","                    break\n","            print(f\"target:     {target_text.replace('-','')}\")\n","            print(f\"prediction: {prediction}\\n\")\n","            \n","batch = next(iter(val_dataset))\n","\n","idx_to_char = list(char_to_num.keys())\n","display_cb = DisplayOutputs(\n","    batch, num_to_char, target_start_token_idx=char_to_num[';'], target_end_token_idx=char_to_num['[']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:57:31.581392Z","iopub.status.busy":"2023-06-12T09:57:31.58065Z","iopub.status.idle":"2023-06-12T09:57:31.719424Z","shell.execute_reply":"2023-06-12T09:57:31.718186Z","shell.execute_reply.started":"2023-06-12T09:57:31.581333Z"},"trusted":true},"outputs":[],"source":["model = Transformer(\n","    num_hid=200,\n","    num_head=2,\n","    num_feed_forward=400,\n","    target_maxlen=64,\n","    num_layers_enc=4,\n","    num_layers_dec=1,\n","    num_classes=59,\n",")\n","\n","loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1,)\n","optimizer = keras.optimizers.Adam(0.0001)\n","model.compile(optimizer=optimizer, loss=loss_fn)"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["    509/Unknown - 94s 150ms/step - loss: 0.9123 - edit_dist: 1.0592"]}],"source":["history = model.fit(train_dataset, validation_data=val_dataset, callbacks=[display_cb], epochs=20)\n","# history = model.fit(train_dataset.take(1), validation_data=val_dataset.take(1), callbacks=[display_cb], epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:58:42.45665Z","iopub.status.busy":"2023-06-12T09:58:42.456176Z","iopub.status.idle":"2023-06-12T09:58:59.898759Z","shell.execute_reply":"2023-06-12T09:58:59.897897Z","shell.execute_reply.started":"2023-06-12T09:58:42.456617Z"},"trusted":true},"outputs":[],"source":["class TFLiteModel(tf.Module):\n","    def __init__(self, model, target_start_token_idx=char_to_num[';'], target_end_token_idx=char_to_num['[']):\n","        super(TFLiteModel, self).__init__()\n","        self.model = model\n","        self.target_start_token_idx = target_start_token_idx\n","        self.target_end_token_idx = target_end_token_idx\n","    \n","    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n","    def __call__(self, inputs, training=False):\n","        # Preprocess Data\n","        x = tf.cast(inputs, tf.float32)\n","        x = x[None]\n","        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n","        x = x[0]\n","        x = pre_process(x)\n","        x = x[None]\n","        x = self.model.generate(x, self.target_start_token_idx)\n","        x = x[0]\n","        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n","        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n","        x = x[1:idx]\n","        x = tf.one_hot(x, 59)\n","        return {'outputs': x}\n","\n","print(pre_process(frames).shape)\n","tflitemodel_base = TFLiteModel(model)\n","tflitemodel_base(frames)[\"outputs\"].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-31T08:12:49.471921Z","iopub.status.busy":"2023-05-31T08:12:49.471505Z","iopub.status.idle":"2023-05-31T08:12:49.605332Z","shell.execute_reply":"2023-05-31T08:12:49.604296Z","shell.execute_reply.started":"2023-05-31T08:12:49.471891Z"},"trusted":true},"outputs":[],"source":["model.save_weights(\"model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-31T08:09:58.894285Z","iopub.status.busy":"2023-05-31T08:09:58.893861Z","iopub.status.idle":"2023-05-31T08:09:59.568622Z","shell.execute_reply":"2023-05-31T08:09:59.567412Z","shell.execute_reply.started":"2023-05-31T08:09:58.894252Z"},"trusted":true},"outputs":[],"source":["#model.load_weights(\"model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T09:59:04.924369Z","iopub.status.busy":"2023-06-12T09:59:04.923905Z","iopub.status.idle":"2023-06-12T10:00:21.971361Z","shell.execute_reply":"2023-06-12T10:00:21.969831Z","shell.execute_reply.started":"2023-06-12T09:59:04.924336Z"},"trusted":true},"outputs":[],"source":["keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n","keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\n","tflite_model = keras_model_converter.convert()\n","\n","with open('/kaggle/working/model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","    \n","with open('inference_args.json', \"w\") as f:\n","    json.dump({\"selected_columns\" : SEL_COLS}, f)\n","    \n","!zip submission.zip  './model.tflite' './inference_args.json'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T10:01:28.509887Z","iopub.status.busy":"2023-06-12T10:01:28.509409Z","iopub.status.idle":"2023-06-12T10:01:34.058306Z","shell.execute_reply":"2023-06-12T10:01:34.056991Z","shell.execute_reply.started":"2023-06-12T10:01:28.509855Z"},"trusted":true},"outputs":[],"source":["interpreter = tf.lite.Interpreter(\"model.tflite\")\n","\n","REQUIRED_SIGNATURE = \"serving_default\"\n","REQUIRED_OUTPUT = \"outputs\"\n","\n","with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","found_signatures = list(interpreter.get_signature_list().keys())\n","\n","if REQUIRED_SIGNATURE not in found_signatures:\n","    raise KernelEvalException('Required input signature not found.')\n","\n","prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n","\n","for frame, target in test_dataset.skip(1000).take(10):\n","    output = prediction_fn(inputs=frame)\n","    prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n","    target = target.numpy().decode(\"utf-8\")\n","    print(\"pred =\", prediction_str, \"; target =\", target)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T10:01:43.651423Z","iopub.status.busy":"2023-06-12T10:01:43.651017Z","iopub.status.idle":"2023-06-12T10:01:56.612493Z","shell.execute_reply":"2023-06-12T10:01:56.610007Z","shell.execute_reply.started":"2023-06-12T10:01:43.651395Z"},"trusted":true},"outputs":[],"source":["%%timeit -n 10\n","output = prediction_fn(inputs=frame)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from Levenshtein import distance\n","\n","scores = []\n","\n","for frame, target in tqdm(test_dataset.take(1000)):\n","    output = prediction_fn(inputs=frame)\n","    prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n","    target = target.numpy().decode(\"utf-8\")\n","    score = (len(target) - distance(prediction_str, target)) / len(target)\n","    scores.append(score)\n","    \n","scores = np.array(scores)\n","print(np.sum(scores) / len(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
